{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254a3b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "import torchvision.transforms as transforms\n",
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "DIM = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67783753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize:\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to tensor\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # Subtract 128 and divide by 128\n",
    "        img = (img * 255.0 - 128.0) / 128.0\n",
    "        return img\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(DIM, interpolation=transforms.InterpolationMode.LANCZOS),  # Resize the image to the desired dimensions\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    # transforms.RandomRotation(45),  # Randomly rotate the image by up to 10 degrees\n",
    "    # transforms.RandomVerticalFlip(),  # Randomly flip the image vertically\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Randomly change brightness, contrast, saturation, and hue\n",
    "    CustomNormalize(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path, device):\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    # Create new OrderedDict without 'module.' prefix\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            name = k[7:] # remove 'module.' prefix\n",
    "        else:\n",
    "            name = k\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    # Load the weights\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab36382",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "embedding_model = ResNet18(128)\n",
    "embedding_model = embedding_model.to(device)\n",
    "embedding_model = load_model(embedding_model, '224x224_ResNet18_AMSoftmax_validation_20250409-172656.pt', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MTCNN face detector\n",
    "mtcnn = MTCNN(keep_all=False, device=device, image_size=112, margin=0)\n",
    "\n",
    "# Function to perform face detection and crop face from image\n",
    "def detect_and_crop_face(image, mtcnn, target_size=(224, 224)):\n",
    "    \"\"\"Detects face, crops using bounding box, makes it square, and resizes to target_size.\"\"\"\n",
    "    \n",
    "    # Convert to PIL\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        image = transforms.ToPILImage()(image)\n",
    "    elif isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        raise ValueError(\"Input image must be a numpy array, torch tensor, or PIL Image.\")\n",
    "\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Detect face\n",
    "    boxes, _ = mtcnn.detect(image)\n",
    "\n",
    "    if boxes is not None:\n",
    "        # Crop the face using the bounding box\n",
    "        x1, y1, x2, y2 = boxes[0].astype(int)\n",
    "        face = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        # If no face is detected, return the original image\n",
    "        print(\"No face detected.\")\n",
    "        return image\n",
    "\n",
    "    # Step 2: Resize to final target size\n",
    "    face = face.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    return face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images from test dataset by iterating over it\n",
    "def get_image_from_path(image_path):\n",
    "    return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def get_test_images(dataset):\n",
    "    test_images = []\n",
    "    for i, sample in zip(tnrange(len(dataset), desc=\"Samples\"), dataset):\n",
    "        image_path = sample[\"filepath\"]\n",
    "        # image = get_image_from_path(image_path)\n",
    "        test_images.append(image_path)\n",
    "    return test_images\n",
    "def get_embeddings_from_images(images, model):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, image in zip(tnrange(len(images), desc=\"Images\"), images):\n",
    "            image = detect_and_crop_face(image, mtcnn, target_size=(224, 224))\n",
    "            image = preprocess(image)\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            embedding = model(image)\n",
    "            embedding = embedding.squeeze()\n",
    "            embedding = embedding.cpu()\n",
    "            embeddings.append(embedding)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5859a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_test = foz.load_zoo_dataset(\"lfw\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = get_test_images(lfw_test)\n",
    "test_embed = get_embeddings_from_images(test_images, embedding_model)\n",
    "print(test_embed[0].shape)\n",
    "test_embeddings = test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ff1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fob.compute_visualization(\n",
    "    lfw_test,      # samples\n",
    "    None,                  # patches_field (set to None if not applicable)\n",
    "    test_embeddings,       # embeddings\n",
    "    label_field=\"ground_truth.label\",\n",
    "    classes=lfw_test.values(\"ground_truth.label\"),\n",
    "    brain_key=\"ResNet_face_detection_embeddings_tsne\",\n",
    "    output_dir=\"ResNet_face_detection_embeddings\",\n",
    "    overwrite=True, \n",
    "    method=\"tsne\",\n",
    ")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    lfw_test,\n",
    "    None,\n",
    "    test_embeddings,    \n",
    "    label_field=\"ground_truth.label\",\n",
    "    classes=lfw_test.values(\"ground_truth.label\"),\n",
    "    brain_key=\"ResNet_face_detection_embeddings_pca\",\n",
    "    output_dir=\"ResNet_face_detection_embeddings\",\n",
    "    overwrite=True,\n",
    "    method = \"pca\",\n",
    ")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    lfw_test,\n",
    "    None,\n",
    "    test_embeddings,\n",
    "    label_field=\"ground_truth.label\",\n",
    "    classes=lfw_test.values(\"ground_truth.label\"),\n",
    "    brain_key=\"ResNet_face_detection_embeddings_umap\",\n",
    "    output_dir=\"ResNet_face_detection_embeddings\",\n",
    "    overwrite=True,\n",
    "    method = \"umap\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = fo.launch_app(lfw_test)\n",
    "sess.open_tab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
