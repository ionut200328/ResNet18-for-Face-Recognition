{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c254a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "# import resnet18 model from pytorch\n",
    "from torchvision.models import resnet18\n",
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260c3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "DIM = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67783753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize:\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to tensor\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # Subtract 128 and divide by 128\n",
    "        img = (img * 255.0 - 128.0) / 128.0\n",
    "        return img\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(DIM, interpolation=transforms.InterpolationMode.LANCZOS),  # Resize the image to the desired dimensions\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    # transforms.RandomRotation(45),  # Randomly rotate the image by up to 10 degrees\n",
    "    # transforms.RandomVerticalFlip(),  # Randomly flip the image vertically\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Randomly change brightness, contrast, saturation, and hue\n",
    "    CustomNormalize(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4257d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can I modify the model to output embeddings of size 128?\n",
    "# 1. Create a new model that outputs embeddings\n",
    "# 2. Modify the last layer of the model to output embeddings\n",
    "# 3. Use a hook to extract embeddings from the model\n",
    "# 4. Use a custom loss function to train the model\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866070b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, path, device):\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    # Create new OrderedDict without 'module.' prefix\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            name = k[7:] # remove 'module.' prefix\n",
    "        else:\n",
    "            name = k\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    # Load the weights\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab36382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./runs/224x224_ResNet18_AMSoftmax_20250409-172656/224x224_ResNet18_AMSoftmax_validation_20250409-172656.pt\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "embedding_model = ResNet18(128)\n",
    "embedding_model = embedding_model.to(device)\n",
    "embedding_model = load_model(embedding_model, './runs/224x224_ResNet18_AMSoftmax_20250409-172656/224x224_ResNet18_AMSoftmax_validation_20250409-172656.pt', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e92a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchsummary.summary(embedding_model, (3, 224, 224), device='cuda')\n",
    "# torchsummary.summary(embedding_model, (3, 112, 96), device='cuda')\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4b425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MTCNN face detector\n",
    "mtcnn = MTCNN(keep_all=False, device='cuda:3', image_size=112, margin=0)\n",
    "\n",
    "# Function to perform face detection and crop face from image\n",
    "def detect_and_crop_face(image, mtcnn, target_size=(224, 224)):\n",
    "    \"\"\"Detects face, crops using bounding box, makes it square, and resizes to target_size.\"\"\"\n",
    "    \n",
    "    # Convert to PIL\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        image = transforms.ToPILImage()(image)\n",
    "    elif isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        raise ValueError(\"Input image must be a numpy array, torch tensor, or PIL Image.\")\n",
    "\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Detect face\n",
    "    boxes, _ = mtcnn.detect(image)\n",
    "\n",
    "    if boxes is not None:\n",
    "        # Crop the face using the bounding box\n",
    "        x1, y1, x2, y2 = boxes[0].astype(int)\n",
    "        face = image.crop((x1, y1, x2, y2))\n",
    "    else:\n",
    "        # If no face is detected, return the original image\n",
    "        print(\"No face detected.\")\n",
    "        return image\n",
    "\n",
    "    # Step 2: Resize to final target size\n",
    "    face = face.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    return face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8826198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images from test dataset by iterating over it\n",
    "def get_image_from_path(image_path):\n",
    "    return Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def get_test_images(dataset):\n",
    "    test_images = []\n",
    "    for i, sample in zip(tnrange(len(dataset), desc=\"Samples\"), dataset):\n",
    "        image_path = sample[\"filepath\"]\n",
    "        # image = get_image_from_path(image_path)\n",
    "        test_images.append(image_path)\n",
    "    return test_images\n",
    "def get_embeddings_from_images(images, model):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, image in zip(tnrange(len(images), desc=\"Images\"), images):\n",
    "            image = detect_and_crop_face(image, mtcnn, target_size=(224, 224))\n",
    "            image = preprocess(image)\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            embedding = model(image)\n",
    "            embedding = embedding.squeeze()\n",
    "            embedding = embedding.cpu()\n",
    "            embeddings.append(embedding)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5859a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train' already downloaded\n",
      "Split 'test' already downloaded\n",
      "Loading 'lfw' split 'train'\n",
      " 100% |███████████████| 9525/9525 [2.6s elapsed, 0s remaining, 3.6K samples/s]      \n",
      "Loading 'lfw' split 'test'\n",
      " 100% |███████████████| 3708/3708 [994.6ms elapsed, 0s remaining, 3.7K samples/s]      \n",
      "Dataset 'lfw' created\n"
     ]
    }
   ],
   "source": [
    "lfw_test = foz.load_zoo_dataset(\"lfw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2c26b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357dbe6fdbdd4533a885c98a79e4e376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Samples:   0%|          | 0/13233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb22c2afccb4a5281a8daabd996f44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images:   0%|          | 0/13233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "test_images = get_test_images(lfw_test)\n",
    "test_embed = get_embeddings_from_images(test_images, embedding_model)\n",
    "print(test_embed[0].shape)\n",
    "test_embeddings = test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728ff1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichitu/machine-learning/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 13233 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 13233 samples in 0.306s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 13233\n",
      "[t-SNE] Computed conditional probabilities for sample 13233 / 13233\n",
      "[t-SNE] Mean sigma: 0.208997\n",
      "[t-SNE] Computed conditional probabilities in 0.181s\n",
      "[t-SNE] Iteration 50: error = 96.6970673, gradient norm = 0.0392076 (50 iterations in 1.084s)\n",
      "[t-SNE] Iteration 100: error = 96.6287155, gradient norm = 0.0489224 (50 iterations in 1.182s)\n",
      "[t-SNE] Iteration 150: error = 96.2231674, gradient norm = 0.0595941 (50 iterations in 1.041s)\n",
      "[t-SNE] Iteration 200: error = 96.3736496, gradient norm = 0.0369489 (50 iterations in 1.218s)\n",
      "[t-SNE] Iteration 250: error = 96.4409561, gradient norm = 0.0762085 (50 iterations in 1.217s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 96.440956\n",
      "[t-SNE] Iteration 300: error = 4.1575871, gradient norm = 0.0102633 (50 iterations in 0.795s)\n",
      "[t-SNE] Iteration 350: error = 3.8050451, gradient norm = 0.0094818 (50 iterations in 0.748s)\n",
      "[t-SNE] Iteration 400: error = 3.6168022, gradient norm = 0.0090125 (50 iterations in 0.675s)\n",
      "[t-SNE] Iteration 450: error = 3.4943726, gradient norm = 0.0085474 (50 iterations in 0.682s)\n",
      "[t-SNE] Iteration 500: error = 3.4079597, gradient norm = 0.0081864 (50 iterations in 0.685s)\n",
      "[t-SNE] Iteration 550: error = 3.3428679, gradient norm = 0.0078156 (50 iterations in 0.689s)\n",
      "[t-SNE] Iteration 600: error = 3.2926028, gradient norm = 0.0074241 (50 iterations in 0.691s)\n",
      "[t-SNE] Iteration 650: error = 3.2530041, gradient norm = 0.0070044 (50 iterations in 0.693s)\n",
      "[t-SNE] Iteration 700: error = 3.2214243, gradient norm = 0.0065690 (50 iterations in 0.690s)\n",
      "[t-SNE] Iteration 750: error = 3.1957467, gradient norm = 0.0062266 (50 iterations in 0.714s)\n",
      "[t-SNE] Iteration 800: error = 3.1744781, gradient norm = 0.0058484 (50 iterations in 0.699s)\n",
      "[t-SNE] Iteration 850: error = 3.1567650, gradient norm = 0.0054513 (50 iterations in 0.700s)\n",
      "[t-SNE] Iteration 900: error = 3.1419065, gradient norm = 0.0050582 (50 iterations in 0.699s)\n",
      "[t-SNE] Iteration 950: error = 3.1293788, gradient norm = 0.0046819 (50 iterations in 0.698s)\n",
      "[t-SNE] Iteration 1000: error = 3.1189032, gradient norm = 0.0042567 (50 iterations in 0.693s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 3.118903\n",
      "Generating visualization...\n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichitu/machine-learning/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP( verbose=True)\n",
      "Tue Apr 29 11:35:50 2025 Construct fuzzy simplicial set\n",
      "Tue Apr 29 11:35:50 2025 Finding Nearest Neighbors\n",
      "Tue Apr 29 11:35:50 2025 Building RP forest with 11 trees\n",
      "Tue Apr 29 11:35:56 2025 NN descent for 14 iterations\n",
      "\t 1  /  14\n",
      "\t 2  /  14\n",
      "\t 3  /  14\n",
      "\t 4  /  14\n",
      "\t 5  /  14\n",
      "\t 6  /  14\n",
      "\t 7  /  14\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Tue Apr 29 11:36:05 2025 Finished Nearest Neighbor Search\n",
      "Tue Apr 29 11:36:07 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce1c807071e4da6b3f0f2fd0eb77e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/200 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  200 epochs\n",
      "\tcompleted  20  /  200 epochs\n",
      "\tcompleted  40  /  200 epochs\n",
      "\tcompleted  60  /  200 epochs\n",
      "\tcompleted  80  /  200 epochs\n",
      "\tcompleted  100  /  200 epochs\n",
      "\tcompleted  120  /  200 epochs\n",
      "\tcompleted  140  /  200 epochs\n",
      "\tcompleted  160  /  200 epochs\n",
      "\tcompleted  180  /  200 epochs\n",
      "Tue Apr 29 11:36:14 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    lfw_test,      # samples\n",
    "    None,                  # patches_field (set to None if not applicable)\n",
    "    test_embeddings,       # embeddings\n",
    "    label_field=\"ground_truth.label\",\n",
    "    classes=lfw_test.values(\"ground_truth.label\"),\n",
    "    brain_key=\"ResNet_face_detection_embeddings_tsne\",\n",
    "    output_dir=\"ResNet_face_detection_embeddings\",\n",
    "    overwrite=True, \n",
    "    method=\"tsne\",\n",
    ")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    lfw_test,\n",
    "    None,\n",
    "    test_embeddings,    \n",
    "    label_field=\"ground_truth.label\",\n",
    "    classes=lfw_test.values(\"ground_truth.label\"),\n",
    "    brain_key=\"ResNet_face_detection_embeddings_pca\",\n",
    "    output_dir=\"ResNet_face_detection_embeddings\",\n",
    "    overwrite=True,\n",
    "    method = \"pca\",\n",
    ")\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    lfw_test,\n",
    "    None,\n",
    "    test_embeddings,\n",
    "    label_field=\"ground_truth.label\",\n",
    "    classes=lfw_test.values(\"ground_truth.label\"),\n",
    "    brain_key=\"ResNet_face_detection_embeddings_umap\",\n",
    "    output_dir=\"ResNet_face_detection_embeddings\",\n",
    "    overwrite=True,\n",
    "    method = \"umap\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2585bf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=2819035f-15fa-4675-a9b3-c7d59e3f6c01\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1e0d348e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "window.open('http://localhost:5151/');",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = fo.launch_app(lfw_test)\n",
    "sess.open_tab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
