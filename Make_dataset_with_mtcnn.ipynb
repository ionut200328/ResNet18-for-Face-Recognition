{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e919695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from PIL import Image, ImageOps\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "# import resnet18 model from pytorch\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import mxnet as mx\n",
    "from mxnet import recordio\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e620ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=False, device='cuda:1', image_size=112, margin=0)\n",
    "\n",
    "def detect_and_crop_face(image, mtcnn, target_size=(224, 224), padding_color=(0, 0, 0)):\n",
    "    \"\"\"Detects face, crops using bounding box, makes it square, and resizes to target_size.\"\"\"\n",
    "    \n",
    "    # Convert to PIL\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        image = transforms.ToPILImage()(image)\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        raise ValueError(\"Input image must be a numpy array, torch tensor, or PIL Image.\")\n",
    "\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Detect face\n",
    "    boxes, _ = mtcnn.detect(image)\n",
    "\n",
    "    if boxes is not None:\n",
    "        face = image.crop((boxes[0][0], boxes[0][1], boxes[0][2], boxes[0][3]))\n",
    "    else:\n",
    "        print(\"No face detected. Using original image.\")\n",
    "        face = image\n",
    "    \n",
    "    face = face.resize(target_size, Image.LANCZOS)\n",
    "    return face\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecCASIAWebFaceDataset(Dataset):\n",
    "    def __init__(self, path_imgrec, transform=None):\n",
    "        self.transform = transform\n",
    "        assert path_imgrec\n",
    "        if path_imgrec:\n",
    "            logging.info('loading recordio %s...',\n",
    "                         path_imgrec)\n",
    "            path_imgidx = path_imgrec[0:-4] + \".idx\"\n",
    "            print(path_imgrec, path_imgidx)\n",
    "            self.imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')\n",
    "            s = self.imgrec.read_idx(0)\n",
    "            header, _ = recordio.unpack(s)\n",
    "            if header.flag > 0:\n",
    "                print('header0 label', header.label)\n",
    "                self.header0 = (int(header.label[0]), int(header.label[1]))\n",
    "                # assert(header.flag==1)\n",
    "                # self.imgidx = range(1, int(header.label[0]))\n",
    "                self.imgidx = []\n",
    "                self.id2range = {}\n",
    "                self.seq_identity = range(int(header.label[0]), int(header.label[1]))\n",
    "                for identity in self.seq_identity:\n",
    "                    s = self.imgrec.read_idx(identity)\n",
    "                    header, _ = recordio.unpack(s)\n",
    "                    a, b = int(header.label[0]), int(header.label[1])\n",
    "                    count = b - a\n",
    "                    self.id2range[identity] = (a, b)\n",
    "                    self.imgidx += range(a, b)\n",
    "                print('id2range', len(self.id2range))\n",
    "            else:\n",
    "                self.imgidx = list(self.imgrec.keys)\n",
    "            self.seq = self.imgidx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Map global index to class ID and local index\n",
    "        actual_idx = idx + 1  # MXNet indices start from 1\n",
    "        \n",
    "        # Read record\n",
    "        header, s = recordio.unpack(self.imgrec.read_idx(actual_idx))\n",
    "        img = mx.image.imdecode(s).asnumpy()\n",
    "        label = int(header.label)\n",
    "        \n",
    "        # # Convert to PIL and apply transforms\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "casia_dataset = RecCASIAWebFaceDataset(path_imgrec='./faces_webface_112x112/train.rec', transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lfw_dataset(path_save):\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "    # load lfw dataset\n",
    "    lfw_dataset = foz.load_zoo_dataset(\"lfw\")\n",
    "    # get the images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    for sample in lfw_dataset:\n",
    "        img = sample.filepath\n",
    "        label = sample['ground_truth']['label']\n",
    "        # remove the label from the image name\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    print(\"images\", len(images))\n",
    "    print(\"labels\", len(labels))\n",
    "    # save the images after mtcnn, with same name as the original image\n",
    "    print(\"save_dir\", path_save)\n",
    "    print(images[0])\n",
    "    print(labels[0])\n",
    "    save_paths = []\n",
    "    for i, (img, label) in enumerate(zip(images, labels)):\n",
    "        label_dir = os.path.join(path_save, str(label))\n",
    "        if not os.path.exists(label_dir):\n",
    "            os.makedirs(label_dir)\n",
    "        img = img.split('/')[-1]\n",
    "        save_paths.append(os.path.join(label_dir, img))\n",
    "    print(\"save_paths\", len(save_paths))\n",
    "    print(\"save_paths\", save_paths[0])\n",
    "    # convert to list of pil images\n",
    "    images = [Image.open(img) for img in images]\n",
    "    # save the images\n",
    "    for i, (img, save_path) in tqdm(enumerate(zip(images, save_paths))):\n",
    "        # Detect and crop face\n",
    "        cropped_face = detect_and_crop_face(img, mtcnn)\n",
    "        # Save the cropped face\n",
    "        if isinstance(cropped_face, torch.Tensor):\n",
    "            cropped_face = transforms.ToPILImage()(cropped_face)\n",
    "        cropped_face.save(save_path)\n",
    "\n",
    "    # delete images of form number_nr.jpg\n",
    "    for label in labels:\n",
    "        label_dir = os.path.join(path_save, str(label))\n",
    "        if os.path.exists(label_dir): #Check to see if labeldir exists\n",
    "            for img_file in os.listdir(label_dir):\n",
    "                if img_file.endswith('_1.jpg') or img_file.endswith('_2.jpg') or img_file.endswith('_3.jpg') or img_file.endswith('_4.jpg'):\n",
    "                    os.remove(os.path.join(label_dir, img_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go thorugh /home/ichitu/py-files/lfw_funneled_cropped and resize every image to 224x224 in a new directory\n",
    "def resize_images_in_directory(input_dir, output_dir, target_size=(224, 224)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img_path = os.path.join(root, file)\n",
    "                img = Image.open(img_path)\n",
    "                img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "                label_dir = os.path.basename(root)\n",
    "                # Create label directory in output path\n",
    "                label_dir_path = os.path.join(output_dir, label_dir)\n",
    "                if not os.path.exists(label_dir_path):\n",
    "                    os.makedirs(label_dir_path)\n",
    "                # Save the resized image\n",
    "                # Use the same filename\n",
    "                # but in the new directory\n",
    "                img.save(os.path.join(label_dir_path, file))\n",
    "# resize_images_in_directory('./lfw_funneled_cropped', './lfw_funneled_cropped_224x224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_casia_dataset(casia_dataset, path_save):\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "    i = 0\n",
    "    for sample in tqdm(casia_dataset):\n",
    "        img = sample[0]\n",
    "        label = sample[1]\n",
    "        \n",
    "        face = detect_and_crop_face(img, mtcnn)\n",
    "        save_path = os.path.join(path_save, str(label))\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        img_name = os.path.join(save_path, str(i) + '.jpg')\n",
    "        face.save(img_name)\n",
    "        i += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_casia_dataset(casia_dataset, './casia_webface_224x224_cropped_mtcnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
